{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Solver (CSP + LLM)\n",
    "\n",
    "Our Hybrid Solver, is a solver that combines Constraint Satisfaction Problem (CSP) techniques with a Language Learning Model (LLM) to efficiently solve Wordle puzzles. Here's how it works:\n",
    "\n",
    "1. Language Agent (LLM Component)\n",
    "    - Uses OpenAI's model (GPT 4o) to make strategic word selections\n",
    "    - Analyzes patterns and letter distributions in remaining candidates\n",
    "    - Calculates information gain for potential guesses\n",
    "    - Provides explanations for word choices\n",
    "\n",
    "2. CSP Component:\n",
    "    - The Constraint Satisfaction Problem component handles the logical filtering of the word list.\n",
    "    - Maintains constraints using the WordleConstraints class (green, yellow, grey letters)\n",
    "    - Tracks minimum letter counts to handle duplicate letters correctly,\n",
    "    - Efficiently filters the word list after each guess via the filter_valid_words function,\n",
    "    - Ensures all candidates are valid according to game rules,\n",
    "\n",
    "3. Solving Process in Detail:,\n",
    "    - **Initialization**: Starts with a complete dictionary of valid words,\n",
    "    - **First Move Optimization**: For large dictionaries, uses \\\"crane\\\" as a statistically efficient first guess,\n",
    "    - **Iterative Guessing**:,\n",
    "        * The LLM agent suggests optimal next words through function-calling capabilities,\n",
    "        * Feedback is collected comparing the guess against the target word\n",
    "        * CSP filters the dictionary to maintain only valid candidates\n",
    "        * Process repeats until solution is found or max attempts reached\n",
    "\n",
    "4. Example of Solving Flow:\n",
    "    1. Start with complete dictionary (e.g., 2,315 words),\n",
    "    2. First guess \\\"crane\\\" â†’ get feedback (e.g., ðŸŸ¨â¬›ðŸŸ©â¬›â¬›),\n",
    "    3. CSP filters dictionary to words matching constraints (e.g., 42 words)\n",
    "    4. LLM analyzes remaining candidates, suggests optimal word through function calling\n",
    "    5. Process repeats until solution is found or we reach 6 guesses\n",
    "\n",
    "## How does the LLM make Strategic Decisions\n",
    "1. Strategic Decision Making:\n",
    "    - **Small Candidate Lists** (â‰¤3 words):\n",
    "        * Performs direct information gain calculation for each candidate\n",
    "        * Selects the word that would provide maximum information\n",
    "        - **Larger Candidate Lists**:\n",
    "          * LLM performs deeper analysis using multiple function calls:\n",
    "            - evaluate_information_gain: Calculates expected entropy reduction\n",
    "            - analyze_letter_patterns: Examines letter frequencies and positional patterns\n",
    "            - explain_choice: Provides reasoning behind the selection, which can be seen in the frontend\n",
    "\n",
    "2. Information Gain Calculation:\n",
    "    - Computes initial entropy of remaining word list\n",
    "    - For each potential guess, simulates all possible feedback patterns\n",
    "    - Groups remaining candidates by feedback pattern\n",
    "    - Calculates expected entropy after guess\n",
    "    - Information gain = initial entropy - expected entropy\n",
    "\n",
    "Example:\n",
    "- Imagine we have 8 possible words remaining: [\"crate\", \"grade\",\"crane\", \"trace\", \"space\", \"brace\", \"grace\", \"track\"]\n",
    "\n",
    "If the LLM wants to calculate the information gain for guessing \"crane\":\n",
    "- Calculate initial entropy:\n",
    "    * 8 possible words = logâ‚‚(8) = 3 bits of uncertainty\n",
    "- Simulate feedback for \"crane\" against each possible target:\n",
    "    * For \"crate\": ðŸŸ©ðŸŸ©â¬›ðŸŸ¨â¬› (feedback pattern A)\n",
    "    * For \"grade\": â¬›ðŸŸ©â¬›ðŸŸ¨â¬› (feedback pattern B)\n",
    "    * For \"crane\": ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ© (feedback pattern C)\n",
    "    * For \"trace\": ðŸŸ¨ðŸŸ©â¬›ðŸŸ¨â¬› (feedback pattern D)\n",
    "    * For \"space\": â¬›â¬›ðŸŸ©â¬›â¬› (feedback pattern E)\n",
    "    * For \"brace\": â¬›ðŸŸ©ðŸŸ©â¬›â¬› (feedback pattern F)\n",
    "    * For \"grace\": â¬›ðŸŸ©ðŸŸ©â¬›â¬› (feedback pattern F again)\n",
    "    * For \"track\": â¬›â¬›ðŸŸ©â¬›ðŸŸ¨ (feedback pattern G)\n",
    "- Group words by feedback pattern:\n",
    "    * Pattern A: [\"crate\"] (1 word)\n",
    "    * Pattern B: [\"grade\"] (1 word)\n",
    "    * Pattern C: [\"crane\"] (1 word)\n",
    "    * Pattern D: [\"trace\"] (1 word)\n",
    "    * Pattern E: [\"space\"] (1 word)\n",
    "    * Pattern F: [\"brace\", \"grace\"] (2 words)\n",
    "    * Pattern G: [\"track\"] (1 word)\n",
    "\n",
    "- Calculate expected entropy after guess:\n",
    "    * P(A) = 1/8, entropy = logâ‚‚(1) = 0\n",
    "    * P(B) = 1/8, entropy = logâ‚‚(1) = 0\n",
    "    * P(C) = 1/8, entropy = logâ‚‚(1) = 0\n",
    "    * P(D) = 1/8, entropy = logâ‚‚(1) = 0\n",
    "    * P(E) = 1/8, entropy = logâ‚‚(1) = 0\n",
    "    * P(F) = 2/8, entropy = logâ‚‚(2) = 1\n",
    "    * P(G) = 1/8, entropy = logâ‚‚(1) = 0\n",
    "\n",
    "Expected entropy = (1/8 Ã— 0) + (1/8 Ã— 0) + (1/8 Ã— 0) + (1/8 Ã— 0) + (1/8 Ã— 0) + (2/8 Ã— 1) + (1/8 Ã— 0) = 0.25\n",
    "\n",
    "Information gain = Initial entropy - Expected entropy = 3 - 0.25 = 2.75 bits\n",
    "\n",
    "- This means \"crane\" gives us 2.75 bits of information, which is very good (close to the maximum possible 3 bits). After guessing \"crane\", we'll likely narrow down to just 1 or 2 possible words.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
