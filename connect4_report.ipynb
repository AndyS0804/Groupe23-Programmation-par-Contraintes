{
  "cells": [
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "# Connect 4: Exploring Algorithmic Approaches\n",
     "\n",
     "**Project Context:** Constraint Programming for Connect 4\n",
     "\n",
     "This notebook outlines and explains various algorithmic approaches that can be considered for developing an intelligent agent to play Connect 4. While the core project might involve Constraint Programming (CP) for aspects like state validation, win condition checking, or perhaps even move generation under certain constraints, this document focuses on common game-playing algorithms that could leverage or complement a CP model."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 1. Introduction to Connect 4\n",
     "\n",
     "Connect 4 is a classic two-player connection game in which players first choose a color and then take turns dropping colored discs from the top into a seven-column, six-row vertically suspended grid. The pieces fall straight down, occupying the lowest available space within the column. The objective of the game is to be the first to form a horizontal, vertical, or diagonal line of four of one's own discs."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {
     "jp-MarkdownHeadingCollapsed": true
    },
    "source": [
     "## 2. Constraint Programming (CP) in Connect 4\n",
     "\n",
     "While the primary decision-making logic for our agents might rely on search algorithms (Minimax, Negamax) or learning (Reinforcement Learning), Constraint Programming offers a powerful declarative paradigm to model and enforce the rules and properties of the Connect 4 game. Instead of explicitly coding *how* to check conditions step-by-step, CP allows us to define the conditions (constraints) that must hold true.\n",
     "\n",
     "In the context, CP can be strategically employed for several key tasks:\n",
     "\n",
     "1.  **Board State Representation & Rule Enforcement:**\n",
     "    *   **Model:** The 6x7 board can be represented using CP variables. For example, a 2D array `board[row][col]` where each variable's domain is `{0, 1, 2}` (representing Empty, Player 1, Player 2).\n",
     "    *   **Gravity Constraint:** A fundamental rule. We can define constraints stating that if `board[r][c]` is non-empty (belongs to Player 1 or 2), then `board[r-1][c]` must also be non-empty, unless `r` is the bottom row (r=0). This ensures pieces stack correctly.\n",
     "    *   **Column Capacity:** A constraint can limit the number of non-empty cells in any given column `c` to be at most 6.\n",
     "    *   **Benefit:** While simple checks are often used in practice for these, using CP provides a formal, declarative model of the game's physics, which can be useful for validation or more complex reasoning.\n",
     "\n",
     "2.  **Win Condition Checking:**\n",
     "    *   **Horizontal:** For each player `P` and each possible starting position `(r, c)`, a constraint like: `board[r][c] == P AND board[r][c+1] == P AND board[r][c+2] == P AND board[r][c+3] == P`.\n",
     "    *   **Vertical:** Similarly: `board[r][c] == P AND board[r+1][c] == P AND board[r+2][c] == P AND board[r+3][c] == P`.\n",
     "    *   **Diagonal (Both directions):** Analogous constraints for diagonal lines.\n",
     "    *   **Integration:** This CP-based check is essential for:\n",
     "        *   **Minimax/Negamax:** Determining if a node is a terminal state (win/loss) in the search tree.\n",
     "        *   **Reinforcement Learning:** Determining when an episode ends (game over) and assigning the appropriate terminal reward (+1 for win, -1 for loss) in the environment's `step` function.\n",
     "        *   **All Agents:** Knowing when to stop the game loop.\n",
     "\n",
     "3.  **Valid Move Identification / Generation:**\n",
     "    *   Define constraints for a legal move in column `c`: `c >= 0 AND c < 7` AND `board[5][c] == 0` (assuming row 5 is the top row). The actual row a piece lands in depends on the gravity constraint.\n",
     "    *   **Integration:** Used by the Random agent to pick from legal moves, and by Minimax/Negamax/RL agents to know the available actions from a given state.\n",
     "\n",
     "4.  **Heuristic Feature Definition (Supporting Minimax/Negamax):**\n",
     "    *   CP can define what constitutes strategically important patterns, even if calculating them uses procedural code for speed.\n",
     "    *   **Example:** Define a constraint set representing \"a Player 1 three-in-a-row with open space on both ends\". The heuristic function could then try to count how many solutions exist for this constraint set on the current board.\n",
     "    *   **Threat Detection:** Model \"a column `c` where Player 2 wins if they play there next\". This involves checking if placing Player 2's piece in `c` satisfies a win condition.\n",
     "    *   **Integration:** CP helps formalize the *features* that the procedural `evaluate_board` function will count or check. Running a CP solver *inside* the heuristic for every node might be too slow, but CP informs the design.\n",
     "\n",
     "**Summary of CP's Role:** In this project setup, CP is unlikely to be the main AI decision engine itself. Instead, it acts as a powerful **rule engine and state analyzer**. Its primary roles are:\n",
     "*   **Robustly checking for terminal states (win/loss/draw)**, which is fundamental for all other algorithms.\n",
     "*   **Formally defining game rules and valid states/moves**, ensuring correctness.\n",
     "*   Potentially **defining complex features** used by the heuristics in search algorithms like Minimax/Negamax."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 3. Naive Strategies\n",
     "\n",
     "These strategies represent simpler, often non-optimal approaches to playing Connect 4. They serve as essential baselines for evaluating more complex algorithms."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### 3.1 Random\n",
     "\n",
     "The Random agent embodies the simplest possible strategy. In any given board state, it identifies all legally valid moves (columns that are not full). From this set of valid moves, it selects one column completely at random and drops its piece there. It has no concept of strategy, winning lines, threats, or opponent intentions. Its primary purpose in benchmarking is to provide a lower bound on performance – any reasonable AI should significantly outperform a purely random player."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### 3.2 Rule-Based Heuristic\n",
     "\n",
     "A Rule-Based Heuristic agent uses a predefined set of simple, prioritized rules to select its move. Unlike search algorithms that look ahead, it typically only evaluates the immediate consequences of potential moves based on these rules. Examples of rules might include:\n",
     "\n",
     "1.  **Win:** If a move results in an immediate win, take it.\n",
     "2.  **Block Win:** If the opponent has an immediate winning move, block it.\n",
     "3.  **Set up Win:** If a move creates a guaranteed win on the next turn (e.g., two simultaneous threats), take it.\n",
     "4.  **Block Setup:** Prevent the opponent from setting up a guaranteed win.\n",
     "5.  **Center Preference:** Prefer playing in center columns (often strategically advantageous).\n",
     "6.  **Build Lines:** Prefer moves that extend lines of 2 or 3 discs.\n",
     "\n",
     "The agent checks these rules in order of priority and executes the first one that applies. If no specific rule applies, it might default to a simpler strategy like playing in the first available column (like the 'Always First' agent seen in the benchmarks) or choosing randomly. These agents are typically fast but can be easily exploited by opponents who look further ahead."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 4. Tree Search Algorithms\n",
     "\n",
     "Tree search algorithms are fundamental to game AI. They explore possible future game states by constructing a game tree, where nodes represent board states and edges represent moves. By evaluating the outcomes deep within the tree, they aim to find the optimal move from the current state, assuming the opponent also plays optimally (or near-optimally)."
    ]
   },
   {
    "attachments": {
     "9dc42edc-fc00-4a19-a2fd-701048f135b9.png": {
      "image/png": "image"
     }
    },
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### 4.1 Minimax\n",
     "\n",
     "The **Minimax** algorithm is a cornerstone of game-playing AI, particularly suited for two-player, zero-sum games with perfect information, like **Connect 4**. It provides a systematic way for an AI player to determine the optimal move in any given board state.\n",
     "\n",
     "#### Core Concept: Simulating Connect 4 Futures\n",
     "\n",
     "Minimax operates by exploring a *game tree* representing possible future states of the Connect 4 board.\n",
     "*   **Nodes:** Each node in this tree corresponds to a specific configuration of the Connect 4 grid. The root node is the current board state.\n",
     "*   **Edges:** Each edge represents a legal move – dropping a colored piece into one of the seven columns (provided the column is not full).\n",
     "The algorithm recursively explores branches of this tree, simulating turns down to a predetermined search depth or until a game-ending state is reached.\n",
     "*   **Terminal States:** In Connect 4, these are states where one player has achieved four pieces in a row (horizontally, vertically, or diagonally) resulting in a win/loss, or the board is completely full, resulting in a draw. These terminal states are assigned scores: typically +1 for an AI win, -1 for an opponent win, and 0 for a draw.\n",
     "\n",
     "<center>\n",
     "    <figure>\n",
     "        <img src=\"benchmark_results\\connect-4-game-tree-diagram.png\" width=\"400\">\n",
     "        <figcaption>Example of a minimax Game Tree</figcaption>\n",
     "    </figure>\n",
     "</center>\n",
     "\n",
     "#### Maximizing (AI) vs. Minimizing (Opponent) Players\n",
     "\n",
     "Minimax assumes optimal play from both sides:\n",
     "*   **Max (Our AI):** Aims to *maximize* the final score. When it's the AI's turn to move (represented by a 'Max' level in the tree), it analyzes the scores resulting from dropping a piece in each valid column and chooses the column leading to the highest possible score.\n",
     "*   **Min (The Opponent):** Aims to *minimize* the AI's score. When it's the opponent's turn ('Min' level), the algorithm assumes the opponent will choose the column leading to the lowest possible score for the AI.\n",
     "Scores from deeper levels (or terminal states) are propagated back up the tree. Max nodes take the maximum value of their children, while Min nodes take the minimum.\n",
     "\n",
     "Minimax uses a *heuristic evaluation function* at its maximum search depth. This function estimates the \"goodness\" of a non-terminal board state for the AI based on the number of potential winning lines. The AI ultimately chooses the initial column drop that leads to the branch with the highest backed-up score, assuming the opponent always plays perfectly to counter.\n",
     "\n",
     "#### Optimization: Alpha-Beta Pruning for Connect 4 Efficiency\n",
     "\n",
     "Exploring the full game tree for Connect 4, even a few moves deep, involves analyzing a massive number of board states. **Alpha-Beta Pruning** is an essential optimization that significantly speeds up this process without changing the outcome. It avoids evaluating parts of the game tree that won't influence the final decision. It maintains two key values during the search:\n",
     "*   **Alpha (α):** The best (highest) score the AI (Max) can currently guarantee itself on the path from the root to the current node.\n",
     "*   **Beta (β):** The best (lowest) score the opponent (Min) can currently guarantee forcing on the path from the root to the current node.\n",
     "\n",
     "Pruning happens in two scenarios relevant to Connect 4 evaluations:\n",
     "1.  While exploring the opponent's potential moves (Min node): If the evaluation of a potential opponent move yields a score less than or equal to `alpha`, the AI knows the opponent could force a worse outcome for the AI than one already found (`alpha`). Therefore, the AI (Max) would have avoided reaching this state earlier. The algorithm stops exploring further moves down this specific branch (*alpha cutoff*).\n",
     "2.  While exploring the AI's potential moves (Max node): If the evaluation of a potential AI move yields a score greater than or equal to `beta`, the AI knows this move looks promising, but it also knows the opponent (Min) already has a way to achieve a better (lower) outcome for themselves (`beta`) higher up the tree. Thus, the opponent would never let the game reach this state. The algorithm stops exploring further down this branch (*beta cutoff*).\n",
     "\n",
     "By eliminating redundant calculations for board states that wouldn't be chosen under optimal play, Alpha-Beta Pruning makes deeper searches in Connect 4 computationally feasible, leading to much stronger AI performance."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### 4.2 Negamax\n",
     "\n",
     "#### Concept\n",
     "Negamax is a variant of the Minimax algorithm specifically tailored for zero-sum games (where one player's gain is exactly the other player's loss), like Connect 4. It simplifies the Minimax logic by observing that maximizing the score for the current player is equivalent to minimizing the score for the opponent. The core principle is often expressed as `max(score for Player A) = -max(score for Player B)`.\n",
     "\n",
     "Instead of having separate logic for maximizing and minimizing players, Negamax always attempts to maximize the score from the perspective of the player whose turn it currently is.\n",
     "\n",
     "#### How it Works in Connect 4\n",
     "\n",
     "1.  **Single Recursive Function:** A single function explores the game tree.\n",
     "2.  **Score Negation:** When the function recursively calls itself for the opponent's move, the score returned by that call (which represents the best outcome for the *opponent*) is negated (`-eval_score`). This flips the score back to the perspective of the *current* player.\n",
     "3.  **Maximization:** The function always chooses the move that leads to the maximum score *after* negation.\n",
     "4.  **Alpha-Beta Pruning:** Pruning works similarly to Minimax, but requires careful handling of the bounds. When making the recursive call, the alpha and beta bounds are swapped and negated (`negamax(..., -beta, -alpha, ...)`). This reflects the change in perspective for the opponent.\n",
     "5.  **Heuristic & Terminal Evaluation:** Uses the same heuristic evaluation function (`score_position`) as Minimax. Terminal state scores (win/loss/draw) are calculated relative to the *original* AI player making the top-level call.\n",
     "\n",
     "#### Pros\n",
     "*   **Code Conciseness:** Often results in slightly less code than a traditional Minimax implementation due to the unified recursive logic (no explicit `if maximizing_player:` check).\n",
     "*   **Theoretical Equivalence:** If implemented correctly with the same heuristic, depth, and pruning logic, it yields the exact same decisions as Minimax.\n",
     "\n",
     "#### Cons\n",
     "*   **Conceptual Nuance:** The score negation and alpha-beta bound swapping can be slightly less intuitive to understand initially compared to the explicit Max/Min player roles in Minimax.\n",
     "\n",
     "#### Integration with CP\n",
     "Negamax integrates with CP in the same way as Minimax:\n",
     "*   Uses CP-verified win checks to identify terminal nodes.\n",
     "*   Relies on CP-verified valid move generation to know which branches to explore.\n",
     "*   The heuristic function (`score_position`), while likely procedural for speed, evaluates features (like N-in-a-row, threats) that can be formally defined using CP constraints."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### 4.3 Monte Carlo Tree Search (MCTS)\n",
     "\n",
     "#### Concept\n",
     "Monte Carlo Tree Search is a probabilistic search algorithm often used for game AI. Unlike Minimax/Negamax which attempt exhaustive search up to a fixed depth using heuristics, MCTS uses repeated random simulations (playouts) to estimate the value of different moves. It builds a search tree incrementally, focusing more time on promising areas of the game.\n",
     "\n",
     "#### Key Steps (The MCTS Cycle)\n",
     "MCTS runs for a fixed number of iterations or a time budget. Each iteration involves four phases:\n",
     "\n",
     "1.  **Selection:** Starting from the root node (current board state), traverse down the existing tree by repeatedly selecting child nodes. The selection typically uses a strategy like UCB1 (Upper Confidence Bound 1 for Trees), which balances **exploitation** (choosing nodes that have historically led to good results) and **exploration** (choosing nodes that haven't been visited much).\n",
     "2.  **Expansion:** Once a node is reached that is not fully expanded (i.e., has potential moves not yet added as children in the tree) and is not a terminal state, add one (or more) new child nodes to the tree corresponding to an unexplored move.\n",
     "3.  **Simulation (Playout):** From the newly expanded node (or the selected leaf node if it was terminal), simulate a complete game by making random (or semi-random using a 'default policy') moves for both players until a terminal state (win/loss/draw) is reached.\n",
     "4.  **Backpropagation:** Take the result of the simulation (+1 for win, -1 for loss, 0 for draw, usually relative to the player whose turn it was at the root) and update the statistics (visit count 'N' and value 'Q') of all nodes along the path taken during the Selection phase, from the simulated node back up to the root.\n",
     "\n",
     "#### How it Works in Connect 4\n",
     "\n",
     "*   **Tree Structure:** Each node holds a Connect 4 board state, visit count (N), total outcome value (Q), and links to parent/children.\n",
     "*   **Simulations:** Random or slightly improved playouts determine game outcomes.\n",
     "*   **Move Choice:** After the allocated iterations/time, the algorithm examines the direct children of the root node. The move corresponding to the child node that is considered 'best' (often the most visited, as it's statistically robust) is chosen as the AI's move.\n",
     "\n",
     "#### Pros\n",
     "*   **No Heuristic Required (Baseline):** Basic MCTS only needs win/loss/draw results, not a complex board evaluation function (though heuristics can enhance it).\n",
     "*   **Asymmetric Growth:** Focuses search effort on more promising lines of play.\n",
     "*   **Anytime Algorithm:** Can be stopped at any time (after any number of iterations) and provide the best move found so far.\n",
     "*   **Effective in Complex Games:** Works well even with very large state spaces or branching factors where Minimax becomes infeasible.\n",
     "\n",
     "#### Cons\n",
     "*   **Computationally Intensive:** Requires many simulations (iterations) for strong play.\n",
     "*   **Simulation Quality:** Performance depends on the quality and speed of the simulation phase. Purely random playouts can sometimes miss obvious tactical threats if not run for enough iterations (mitigated by improved default policies).\n",
     "*   **Non-Deterministic (Usually):** Due to the randomness in simulations and potentially in tie-breaking during selection, it might not always choose the same move in the same situation.\n",
     "\n",
     "#### Integration with CP\n",
     "MCTS relies heavily on fast and accurate game rule enforcement, making CP integration valuable:\n",
     "*   **Simulation Termination:** Needs rapid CP-verified win/loss/draw checks to end simulations.\n",
     "*   **Valid Moves:** Requires CP-verified valid move generation during both the Expansion phase (creating new nodes) and the Simulation phase (choosing random/policy moves)."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 5. Expert System (Leo algo)\n",
     "\n",
     "An Expert System approach, encodes human expert knowledge about Connect 4 strategy into a set of rules or heuristics. Unlike simpler rule-based agents, an expert system might incorporate more complex pattern recognition and strategic principles.\n",
     "\n",
     "**Concept:** The core idea is to mimic how a skilled human player might reason about the game. This involves identifying critical board configurations, potential threats, opportunities to create winning lines, and general positional advantages.\n",
     "\n",
     "**How it Might Work in Connect 4:**\n",
     "*   **Knowledge Base:** Contains rules and patterns representing expert strategies (e.g., \"If setting up a double threat is possible, prioritize it\", \"Avoid placing a piece below an opponent's potential winning spot if it enables their win\", \"Recognize and counter common trap patterns\").\n",
     "*   **Inference Engine:** Applies the rules from the knowledge base to the current board state to evaluate potential moves.\n",
     "*   **Pattern Matching:** May involve sophisticated algorithms to detect complex arrangements of pieces that signal threats or opportunities (e.g., identifying potential 'forks' or 'ZUG patterns').\n",
     "*   **Move Selection:** Chooses the move deemed best according to the evaluation performed by the inference engine based on the applied rules and recognized patterns.\n",
     "\n",
     "**Pros:**\n",
     "*   **Strong Performance (Potentially):** Can achieve high performance if the encoded knowledge is comprehensive and accurate.\n",
     "*   **Explainability (Potentially):** The reasoning behind a move might be traceable back to specific rules, making it more understandable than purely learned models.\n",
     "\n",
     "**Cons:**\n",
     "*   **Knowledge Engineering Bottleneck:** Creating and refining the expert rules is time-consuming and requires deep domain expertise.\n",
     "*   **Brittleness:** May perform poorly in situations not explicitly covered by the rules.\n",
     "*   **Complexity:** Implementing sophisticated pattern matching and rule inference can be complex.\n",
     "\n",
     "**Integration with CP:** CP can assist in formally defining the complex patterns and conditions that the expert system needs to recognize, making the rule implementation more robust."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 6. Pattern-Based Agent\n",
     "\n",
     "A Pattern-Based agent focuses specifically on recognizing predefined geometric or strategic patterns on the Connect 4 board to guide its move selection. It's closely related to Expert Systems and Rule-Based Heuristics but emphasizes the detection of specific configurations of discs.\n",
     "\n",
     "**Concept:** The agent maintains a library of known patterns, each associated with a strategic value or action. It scans the board for occurrences of these patterns and chooses a move based on the presence and significance of detected patterns.\n",
     "\n",
     "**How it Might Work in Connect 4:**\n",
     "*   **Pattern Library:** Contains representations of patterns like:\n",
     "    *   Immediate winning lines (3-in-a-row with an open space).\n",
     "    *   Opponent's immediate threats.\n",
     "    *   Potential forks (setting up two simultaneous winning threats).\n",
     "    *   Common traps or sequences.\n",
     "    *   Lines of 2 or 3 with open ends.\n",
     "*   **Board Scanning:** Algorithms to efficiently search the current board for instances of patterns in the library.\n",
     "*   **Evaluation/Action:** Each pattern might have a score or a recommended move (e.g., play in the winning spot, block the threat, play in the fork-creating spot). The agent aggregates this information, possibly prioritizing certain patterns over others, to select its final move.\n",
     "\n",
     "**Pros:**\n",
     "*   **Targeted Strategy:** Can be very effective at executing specific tactics and recognizing common threats.\n",
     "*   **Efficiency:** Pattern matching can sometimes be faster than deep tree searches for specific tactical situations.\n",
     "\n",
     "**Cons:**\n",
     "*   **Incompleteness:** The agent's strength is limited by the comprehensiveness of its pattern library. It might miss novel situations or long-term strategic considerations not captured by patterns.\n",
     "*   **Development Effort:** Defining and implementing the pattern detection logic requires careful design and potentially significant effort.\n",
     "\n",
     "**Integration with CP:** CP can be used to formally define the patterns themselves as constraint satisfaction problems, aiding in their precise identification on the board."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 7. Genetic Algorithm Agent\n",
     "\n",
     "A Genetic Algorithm (GA) approach uses principles inspired by biological evolution to discover effective strategies or parameters for playing Connect 4. Instead of relying on predefined rules or direct game tree search during play, a GA typically optimizes components of a strategy offline.\n",
     "\n",
     "**Concept:**\n",
     "GAs operate on a population of 'individuals', where each individual represents a potential solution or strategy component. These individuals are evaluated based on their 'fitness' – typically how well they perform in playing Connect 4. Fitter individuals are preferentially selected to 'reproduce', creating a new generation of solutions by combining aspects of the parents (crossover) and introducing small random changes (mutation). Over many generations, this process tends to evolve the population towards higher-performing solutions.\n",
     "\n",
     "**How it Might Work in Connect 4:**\n",
     "*   **Representation (Chromosome):** An individual's 'chromosome' needs to encode a strategy. Common approaches include:\n",
     "    *   **Heuristic Function Weights:** The chromosome could be a set of numerical weights assigned to different features of the board state (e.g., the value of having two pieces in a row, three pieces in a row, controlling the center column, blocking opponent threats). The agent then uses a function that calculates a score for potential moves based on these weighted features.\n",
     "    *   **Rule-Based Parameters:** If the agent uses a set of rules, the chromosome could encode parameters that control how these rules are applied or prioritized.\n",
     "    *   **(Advanced) Policy Representation:** It could encode parameters of a function (like a neural network) that directly maps board states to moves.\n",
     "*   **Fitness Evaluation (Offline):** The core of the GA process. To determine how 'fit' a set of weights or parameters is, an agent using them must play many games. Opponents could be other individuals in the population, fixed benchmark agents, or previous versions of the evolving agent. Fitness is usually measured by win rate or average score achieved over these games.\n",
     "*   **Evolutionary Operators:** Standard GA operators are applied iteratively:\n",
     "    *   **Selection:** Choosing fitter individuals to become parents.\n",
     "    *   **Crossover:** Creating offspring by combining genetic material (e.g., parts of the weight vectors) from two parents.\n",
     "    *   **Mutation:** Introducing small, random changes to offspring's chromosomes (e.g., slightly altering a weight) to maintain diversity and explore new possibilities.\n",
     "\n",
     "**Runtime Agent Behavior:**\n",
     "Once the offline GA process has converged or run for a sufficient time, the best-evolved chromosome (e.g., the most effective set of heuristic weights) is extracted. The runtime agent then uses this *fixed* set of parameters to make decisions during a game. Typically, it evaluates all valid moves by applying the evolved heuristic function to the resulting board states and selects the move leading to the best score, possibly incorporating basic checks for immediate wins or losses.\n",
     "\n",
     "**Pros:**\n",
     "*   **Optimization Power:** Effective at finding good sets of parameters within a defined strategy space (like weights for a heuristic).\n",
     "*   **Discovery:** Can potentially discover non-obvious interactions between strategic features or effective parameter balances.\n",
     "*   **Adaptability (during training):** The population adapts over generations to the challenges posed by opponents used in fitness evaluation.\n",
     "\n",
     "**Cons:**\n",
     "*   **Computational Cost (Training):** The evolutionary process requires significant computation, involving potentially millions of game simulations for fitness evaluation.\n",
     "*   **Representation Sensitivity:** The effectiveness heavily depends on choosing a good way to represent the strategy (the chromosome) and defining relevant features (if evolving heuristic weights).\n",
     "*   **Parameter Tuning:** GAs have their own set of parameters (population size, crossover/mutation rates) that require tuning for optimal performance.\n",
     "*   **Static Runtime Strategy:** Once trained, the agent uses fixed parameters; it doesn't adapt further during a single game beyond its heuristic evaluation.\n",
     "\n",
     "**Integration with CP:** Constraint Programming is vital for the fitness evaluation phase within the GA, ensuring that the numerous simulated games adhere strictly to Connect 4 rules (valid moves, win/loss/draw detection)."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 8. Reinforcement Learning (RL)\n",
     "\n",
     "### Concept\n",
     "Reinforcement Learning is a machine learning paradigm where an agent learns to make decisions by taking actions in an environment to maximize a cumulative reward signal. The agent isn't told *what* action to take, but instead discovers which actions yield the most reward through trial and error.\n",
     "\n",
     "### Key Components\n",
     "*   **Agent:** The learner or decision-maker (our Connect 4 player).\n",
     "*   **Environment:** The external system the agent interacts with (the Connect 4 game).\n",
     "*   **State (s):** A representation of the environment's current situation (the board configuration).\n",
     "*   **Action (a):** A choice the agent can make (dropping a disc in a column).\n",
     "*   **Reward (r):** Feedback from the environment indicating the immediate consequence of an action (e.g., +1 for winning, -1 for losing, 0 for other moves).\n",
     "*   **Policy (π):** The agent's strategy for choosing actions based on states.\n",
     "*   **Value Function (V(s) or Q(s,a)):** Estimates the expected long-term return (cumulative reward) from a state or state-action pair.\n",
     "\n",
     "### Relevance to Connect 4\n",
     "RL allows an agent to learn to play Connect 4 potentially without prior knowledge of optimal strategies, simply by playing many games (often against itself or variations of itself) and learning from the outcomes."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### 8.1 Deep Q-Learning (DQN)\n",
     "\n",
     "Deep Q-Learning is a popular RL algorithm that uses a deep neural network to approximate the optimal action-value function, known as Q*(s, a). This function represents the maximum expected future reward achievable from state 's' by taking action 'a' and following the optimal policy thereafter.\n",
     "\n",
     "**How it Works (Briefly):**\n",
     "\n",
     "1.  **Q-Network:** A neural network takes the current state 's' as input and outputs estimated Q-values for each possible action 'a' in that state.\n",
     "2.  **Experience Replay:** The agent's experiences (state, action, reward, next_state) tuples are stored in a memory buffer. During training, mini-batches are randomly sampled from this buffer. This breaks temporal correlations and improves learning stability.\n",
     "3.  **Target Network:** A separate neural network (a periodically updated copy of the main Q-network) is used to calculate the target Q-values for the learning update. This target is `r + γ * max_a'(Q_target(s', a'))`, where `γ` (gamma) is the discount factor for future rewards, `s'` is the next state, and `Q_target` is the value from the target network. Using a fixed target network for a period stabilizes training.\n",
     "4.  **Learning:** The main Q-network is updated using gradient descent to minimize the difference (e.g., Mean Squared Error) between its predicted Q(s, a) and the calculated target Q-value.\n",
     "5.  **Exploration:** An exploration strategy (like epsilon-greedy, where the agent takes a random action with probability epsilon, otherwise chooses the action with the highest Q-value) is used to balance exploring new actions and exploiting known good actions."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "#### 8.1.1 DQN with a Simple Neural Network (MLP)\n",
     "\n",
     "*   **Input:** The Connect 4 board (6x7 grid) is typically flattened into a 1D vector (e.g., 42 elements). Each element represents a cell, possibly encoded as 0 (empty), 1 (player 1), -1 (player 2).\n",
     "*   **Network:** A Multi-Layer Perceptron (MLP) with one or more hidden dense layers (using activation functions like ReLU) followed by an output layer.\n",
     "*   **Output:** The output layer has 7 neurons, one for each column. The value of each output neuron represents the estimated Q-value for dropping a disc in that column.\n",
     "*   **Pros:** Relatively simple to implement.\n",
     "*   **Cons:** Flattening the input loses the spatial relationships between cells (adjacency, lines), which are crucial in Connect 4. The network must learn these relationships implicitly, which can be inefficient."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "#### 8.1.2 DQN with a Convolutional Neural Network (CNN)\n",
     "\n",
     "*   **Input:** The board is treated as a 2D grid (e.g., 6x7). Often, multiple input *channels* are used to represent the state more effectively (e.g., one 6x7 channel indicating positions of player 1's pieces, another for player 2's pieces, maybe a third channel indicating whose turn it is).\n",
     "*   **Network:** Starts with one or more convolutional layers. These layers use filters (kernels) to detect spatial patterns (like horizontal, vertical, diagonal lines, or local configurations) across the board. Pooling layers might be used to reduce dimensionality. The output from the convolutional/pooling layers is then typically flattened and fed into one or more dense layers (MLP style).\n",
     "*   **Output:** Similar to the MLP approach, the final output layer has 7 neurons representing the Q-values for each column.\n",
     "*   **Pros:** CNNs are designed to recognize spatial hierarchies and patterns, making them well-suited for board games like Connect 4. They can learn relevant features more efficiently than MLPs processing flattened input.\n",
     "*   **Cons:** More complex architecture and more computationally intensive to train than a simple MLP."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# Code cell for potential DQN structure (pseudo-code or library calls)\n",
     "# E.g. (Conceptual):\n",
     "# model = create_cnn_q_network(input_shape=(6, 7, num_channels), output_size=7)\n",
     "# target_model = create_cnn_q_network(input_shape=(6, 7, num_channels), output_size=7)\n",
     "# replay_buffer = ReplayBuffer(capacity=10000)\n",
     "# optimizer = Adam(learning_rate=0.001)\n",
     "# \n",
     "# for episode in range(num_episodes):\n",
     "#     state = env.reset()\n",
     "#     done = False\n",
     "#     while not done:\n",
     "#         action = select_action_epsilon_greedy(state, model, epsilon)\n",
     "#         next_state, reward, done, _ = env.step(action)\n",
     "#         replay_buffer.add(state, action, reward, next_state, done)\n",
     "#         state = next_state\n",
     "#         \n",
     "#         if len(replay_buffer) > batch_size:\n",
     "#             sample_batch = replay_buffer.sample(batch_size)\n",
     "#             train_step(model, target_model, sample_batch, optimizer, gamma)\n",
     "#             \n",
     "#     # Update target network periodically\n",
     "#     if episode % target_update_frequency == 0:\n",
     "#         target_model.set_weights(model.get_weights())\n",
     "#     \n",
     "#     # Decay epsilon, log metrics, etc."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 9. Performance Analysis and Benchmarking\n",
     "\n",
     "This section presents the results of benchmarking various Connect 4 AI agents against each other. Each agent played a total of 140 games, split evenly between playing as Player 1 (starting player) and Player 2. In pairwise matchups, each agent played 10 games against every other agent as Player 1 and 10 games as Player 2.\n",
     "\n",
     "### Overall Performance Ranking\n",
     "\n",
     "The overall performance, ranked by win percentage across all 140 games played by each agent, is shown below:\n",
     "\n",
     "```text\n",
     "| Rank | AI Algorithm    | Win % | Wins | Losses | Draws | Total Games |\n",
     "|------|-----------------|-------|------|--------|-------|-------------|\n",
     "| 1    | Minimax         | 100.0 | 140  | 0      | 0     | 140         |\n",
     "| 2    | Genetic         | 75.0  | 105  | 34     | 1     | 140         |\n",
     "| 3    | Expert System   | 65.7  | 92   | 48     | 0     | 140         |\n",
     "| 4    | MonteCarlo      | 60.0  | 84   | 55     | 1     | 140         |\n",
     "| 5    | Pattern-Based   | 54.3  | 76   | 64     | 0     | 140         |\n",
     "| 6    | Negamax         | 27.9  | 39   | 101    | 0     | 140         |\n",
     "| 7    | Always First    | 11.4  | 16   | 124    | 0     | 140         |\n",
     "| 8    | Random          | 5.0   | 7    | 133    | 0     | 140         |\n",
     "```\n",
     "\n",
     "\n",
     "**Observations:**\n",
     "*   **Minimax Dominance:** The Minimax agent achieved a perfect 100% win rate, demonstrating its optimal play in this deterministic game against the tested opponents.\n",
     "*   **Strong Performers:** The Genetic algorithm and the Expert System ('Leo algo') performed very well, securing 75% and 65.7% win rates respectively.\n",
     "*   **Mid-Tier:** Monte Carlo and Pattern-Based agents show respectable performance around the 50-60% win rate mark.\n",
     "*   **Weak Performers:** Negamax (likely due to implementation details or search depth compared to Minimax), Always First, and Random agents performed poorly, as expected for simpler or flawed strategies.\n",
     "\n",
     "<center>\n",
     "    <figure>\n",
     "        <img src=\"benchmark_results\\performance.png\" width=\"800\">\n",
     "    </figure>\n",
     "</center>\n"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### Pairwise Matchup Performance\n",
     "\n",
     "The heatmap below shows the win percentage of the Row player (playing as P1) against the Column player (playing as P2) across 10 games for each specific matchup.\n",
     "\n",
     "<center>\n",
     "    <figure>\n",
     "        <img src=\"benchmark_results\\heatmap.png\" width=\"800\">\n",
     "    </figure>\n",
     "</center>\n",
     "\n",
     "\n",
     "**Observations:**\n",
     "*   **Minimax Unbeatable:** The Minimax agent won 100% of its games, regardless of whether it played as P1 or P2 (the 0% entries in the Minimax *row* indicate the *opponent's* win rate when Minimax played as P2 was 0%).\n",
     "*   **Strong vs. Weak:** Genetic, Leo algo, MonteCarlo, and Pattern-Based agents consistently achieve high win rates (often 100%) against the weaker Always First, Negamax, and Random agents.\n",
     "*   **Competitive Matchups:** \n",
     "    *   Genetic vs MonteCarlo: Genetic wins 60% as P1.\n",
     "    *   Leo algo vs MonteCarlo: Leo algo wins 70% as P1.\n",
     "    *   MonteCarlo vs Leo algo: 50% win rate (likely P2 wins here).\n",
     "    *   Pattern-Based vs Leo algo: Pattern-Based wins 100% as P1.\n",
     "    *   Pattern-Based vs MonteCarlo: Pattern-Based wins 70% as P1.\n",
     "*   **Negamax Struggles:** The Negamax agent loses most matchups, only showing strength against Random (90% win rate as P1).\n",
     "*   **Always First & Random:** These baseline agents perform very poorly against all but each other (Always First beats Random 70% of the time as P1).\n"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "### Benchmarking Summary\n",
     "\n",
     "The benchmark results clearly establish Minimax as the superior algorithm in this set, achieving perfect play. Genetic, Leo algo (Expert System), MonteCarlo, and Pattern-Based agents form a competitive middle tier, demonstrating significantly better-than-random strategies. Negamax, Always First, and Random lag considerably behind. The near-absence of errors suggests reliable agent implementations, with the single Monte Carlo timeout being a minor exception possibly related to its computational demands."
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 10. Bibliography\n",
     "\n",
     "### Minimax\n",
     "* https://en.wikipedia.org/wiki/Minimax\n",
     "* https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning\n",
     "\n",
     "### RL\n",
     "* https://fr.wikipedia.org/wiki/Q-learning\n",
     "* https://huggingface.co/learn/deep-rl-course/unit3/deep-q-algorithm\n",
     "* https://www.youtube.com/watch?v=U9nkd2jt3b8\n"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": []
   }
  ],
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3 (ipykernel)",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.13.2"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 4
 }